apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: <USER>-<JOB_NAME>
spec:
  replicatedJobs:
  - name: workers
    template:
      spec:
        parallelism: <NODE_COUNT>
        completions: <NODE_COUNT>
        backoffLimit: 0
        template:
          metadata:
             annotations:
               gke-gcsfuse/volumes: "true"
               gke-gcsfuse/memory-limit: "0"
               gke-gcsfuse/ephemeral-storage-limit: "0"
               gke-gcsfuse/cpu-limit: "0"

          spec:
            schedulingGates:
            - name: "gke.io/topology-aware-auto-scheduling-<USER>-<JOB_NAME>"
            hostNetwork: true
            dnsPolicy: ClusterFirstWithHostNet
            nodeSelector:
               cloud.google.com/gke-accelerator: nvidia-h100-mega-80gb
            serviceAccountName: training
            volumes:
            - name: libraries
              hostPath:
                path: /home/kubernetes/bin/nvidia/lib64

            - name: nvidia-install-dir-host
              hostPath:
                path: /home/kubernetes/bin/nvidia

            - name: gcs-pvc
              persistentVolumeClaim:
               claimName: training-data
               readOnly: false

            # - name: gcs-fuse-csi-ephemeral
            #   csi:
            #     driver: gcsfuse.csi.storage.gke.io
            #     readOnly: false
            #     volumeAttributes:
            #        bucketName: irreverent-datasets
            #        mountOptions: "implicit-dirs"
            #        gcsfuseLoggingSeverity: warning
            #        fileCacheCapacity: "200Gi"
                  

            - name: dshm
              emptyDir:
                medium: Memory

            containers:
            - name: tcpxo-daemon
              image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/tcpgpudmarxd-dev:v1.0.9
              imagePullPolicy: Always
              command: ["/bin/sh", "-c"]
              args:
                - |
                  set -ex
                  chmod 755 /fts/entrypoint_rxdm_container.sh
                  /fts/entrypoint_rxdm_container.sh --num_hops=2 --num_nics=8 --uid= --alsologtostderr
              securityContext:
                privileged: true
              volumeMounts:
                - name: nvidia-install-dir-host
                  mountPath: /usr/local/nvidia
              env:
                - name: LD_LIBRARY_PATH
                  value: /usr/local/nvidia/lib64

            - name: pytorch
              image: us-west4-docker.pkg.dev/production-cluster-424922/docker-images/experiments:<USER>-<DOCKER_TAG>
              imagePullPolicy: Always
              # image: gcr.io/k8s-staging-jobset/pytorch-mnist:latest
              ports:
              - containerPort: 3389
              env:
              - name: MASTER_ADDR
                value: "<USER>-<JOB_NAME>-workers-0-0.<USER>-<JOB_NAME>"
              - name: MASTER_PORT
                value: "3389"
              - name: NODE_COUNT
                value: "<NODE_COUNT>"
              - name: NODE_RANK
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
              # Force python to not buffer output and write directly to stdout, so we can view training logs via `kubectl logs`.
              - name: PYTHONUNBUFFERED
                value: "0"    
              - name: WANDB_API_KEY
                valueFrom:
                  secretKeyRef:
                    name: wandb-api-key-<USER>
                    key: WANDB_API_KEY                
              - name: PYTHON_SCRIPT
                value: "experiments/diffusion/diffusion.py --config=<CONFIG_FILE> --trainer.num_nodes=<NODE_COUNT> --trainer.logger.name=<JOB_NAME> --trainer.devices=auto <CONFIG_OPTIONS> fit"
              - name: NCCL_DEBUG
                value: INFO
              - name: NCLL_DEBUG_SUBSYS
                value: ALL
              - name: LD_LIBRARY_PATH
                value: /usr/local/nvidia/lib64  
              # - name: TRITON_CACHE_DIR
              #   value: irreverent-datasets/torch-compiler-cache
              securityContext:
                privileged: true    
              command: ["/bin/bash", "-c"]
              args: ["/root/run.sh"]         
              # args: ["/root/run_multi_node_profile.sh"]         
              # args: ["-c", "while true; do echo hello; sleep 10;done"]                             
              resources:
                requests:
                  cpu: "8"
                  memory: "25Gi"
                  ephemeral-storage: "25Gi"
                  nvidia.com/gpu: 8
                limits:
                  # cpu: "16"
                  # memory: "30Gi"
                  # ephemeral-storage: "30Gi"
                  nvidia.com/gpu: 8

              volumeMounts:
               - mountPath: /dev/shm
                 name: dshm
              #  - mountPath: /irreverent-datasets
              #    name: gcs-pv
               - mountPath: /irreverent-datasets
                 name: gcs-pvc
                 

